{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zUy2qrq0r3n"
      },
      "outputs": [],
      "source": [
        "\n",
        "import chess\n",
        "from chess.pgn import Game\n",
        "\n",
        "import requests\n",
        "import bz2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Input, Dense, Flatten, Concatenate, Conv2D\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.models import Model, clone_model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxslGv_refdk"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "La librairie py-chess permet de générer, visualiser et mouvoir les pièces d'un échiquier et aussi de valider la configuration d'où son importance pour notre génération de configurations d'échiquiers par modèle génératif plus tard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHhD8WBT0vYP"
      },
      "outputs": [],
      "source": [
        "board = chess.Board()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwC7H2-T0wdn"
      },
      "outputs": [],
      "source": [
        "board"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_qAmDxEe7Ap"
      },
      "source": [
        "# Data retrieval and processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data_from_the_web(url='https://database.lichess.org/standard/lichess_db_standard_rated_2013-05.pgn.bz2') :\n",
        "  \n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  decompressed_r = bz2.decompress(r.content)\n",
        "  open('chess_data_sample.txt', 'wb').write(decompressed_r)\n",
        "  with open('chess_data_sample.txt', 'rb') as f:\n",
        "    lines = f.readlines()\n",
        "  \n",
        "  moves = []\n",
        "  for l in lines :\n",
        "    move = l.decode('utf-8')\n",
        "    if move[0] != '[' and move[0] != '\\n' and not '{' in move :\n",
        "      moves.append(move)\n",
        "\n",
        "  return moves"
      ],
      "metadata": {
        "id": "xqfVdF0YoC1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7ArwQY8J3Pd"
      },
      "outputs": [],
      "source": [
        "moves = download_data_from_the_web(url='https://database.lichess.org/standard/lichess_db_standard_rated_2013-05.pgn.bz2')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding the chessboard"
      ],
      "metadata": {
        "id": "bCz4uDDqM5Yn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-TnGaEgKp0-"
      },
      "outputs": [],
      "source": [
        "def create_board(sequence_moves, max_moves=5) :\n",
        "  \n",
        "  board = chess.Board()\n",
        "  moves_list = list(map(lambda x : x.strip(), re.split(r'(?:\\d){1,2}\\.', sequence_moves)))\n",
        "  moves_list = moves_list[1:len(moves_list)-1]\n",
        "  \n",
        "  for t in moves_list[0:max_moves] : \n",
        "    board.push_san(t.split()[0])\n",
        "    board.push_san(t.split()[1])\n",
        "    \n",
        "  return board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOQJ6bungL1R"
      },
      "outputs": [],
      "source": [
        "def fen_to_matrix(board):\n",
        "\n",
        "  pieces = ['r', 'n', 'b', 'q', 'k', 'p'] + list(map(lambda x : x.upper(), ['r', 'n', 'b', 'q', 'k', 'p']))\n",
        "  list_arrays = [ np.eye(12)[:, i] for i in range(12) ]\n",
        "  encoding_dict = { k : v for k, v in zip(pieces, list_arrays) }\n",
        "  fen_list = board.fen().split()[0].split('/')\n",
        "  M = np.zeros((12, 8, 8))\n",
        "  for i, row in enumerate(fen_list) :\n",
        "    j = 0\n",
        "    for e in row :\n",
        "      if e.isdigit() :\n",
        "        j+= int(e)\n",
        "      else :\n",
        "         M[:, i, j] = encoding_dict[e]\n",
        "         j+=1\n",
        "  return M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWijX8v1vaaE"
      },
      "source": [
        "### Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpqIuMLkvtnA"
      },
      "outputs": [],
      "source": [
        "def get_X_X(moves):\n",
        "  X = [0]*len(moves)\n",
        "  for i, move in enumerate(moves):\n",
        "    #print(i, move)\n",
        "    board = create_board(sequence_moves=move)\n",
        "    M = fen_to_matrix(board)\n",
        "    X[i] = M\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBVrQlYwxyvX"
      },
      "outputs": [],
      "source": [
        "X = get_X_X(moves)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH70FnBlxO6N"
      },
      "source": [
        "# Variational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build"
      ],
      "metadata": {
        "id": "INUV6l9CPI37"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pajd6Kg2rp6"
      },
      "outputs": [],
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mikvtz5-2frP"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, beta, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = self.init_encoder()\n",
        "        self.decoder = self.init_decoder()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "        self.beta = beta\n",
        "\n",
        "    def init_encoder(self):\n",
        "      latent_dim = 2\n",
        "      encoder_inputs = keras.Input(shape=(12, 8, 8))\n",
        "      x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "      x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "      x = layers.Flatten()(x)\n",
        "      x = layers.Dense(16, activation=\"relu\")(x)\n",
        "      z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "      z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "      z = Sampling()([z_mean, z_log_var])\n",
        "      encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "      return encoder\n",
        "\n",
        "    def init_decoder(self):\n",
        "      latent_dim = 2\n",
        "      latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "      x = layers.Dense(3 * 2 * 64, activation=\"relu\")(latent_inputs)\n",
        "      x = layers.Reshape((3, 2, 64))(x)\n",
        "      x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "      x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "      decoder_outputs = layers.Conv2DTranspose(8, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "      decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "      return decoder\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            print(reconstruction.get_shape().as_list())\n",
        "            print(data.get_shape().as_list())\n",
        "            reconstruction_loss = tf.reduce_sum(\n",
        "                keras.losses.binary_crossentropy(data, reconstruction, axis=0), axis=[0, 1, 2]\n",
        "                )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Step run during validation.\"\"\"\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "\n",
        "        \n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        # For test we use only z_mean :\n",
        "        reconstruction = self.decoder(z_mean)\n",
        "        reconstruction_loss = tf.reduce_sum(\n",
        "            tf.keras.losses.binary_crossentropy(data, reconstruction, axis=0), axis=[0, 1, 2]\n",
        "             )\n",
        "        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "        #print(kl_loss.get_shape().as_list())\n",
        "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "        #print(kl_loss.get_shape().as_list())\n",
        "        kl_loss *= -0.5\n",
        "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
        "      \n",
        "        return {\n",
        "            \"loss\": total_loss,\n",
        "            \"reconstruction_loss\": reconstruction_loss,\n",
        "            \"kl_loss\": kl_loss,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "k6s3xdoIPLwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDpr9v1z2jXF"
      },
      "outputs": [],
      "source": [
        "x_data = np.array(X[:int(0.7 * len(X))])\n",
        "x_val = np.array(X[int(0.7 * len(X)):])\n",
        "\n",
        "x_data = x_data.astype('float32')\n",
        "x_val = x_val.astype('float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMsdf5vy7Bj1"
      },
      "outputs": [],
      "source": [
        "vae = VAE(beta=1/1000)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "history = vae.fit(x_data, epochs=100, batch_size=64, validation_data=(x_val, x_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIhSRDG1un4V"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['reconstruction_loss'])\n",
        "plt.plot(history.history['val_reconstruction_loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Reconstruction loss(BCE)')\n",
        "plt.legend(['Train', 'Val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIQShuUKE1mg"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['kl_loss'])\n",
        "plt.plot(history.history['val_kl_loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('KL loss')\n",
        "plt.legend(['Train', 'Val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBHR8xmf_P7K"
      },
      "outputs": [],
      "source": [
        "def plot_latent_space(vae, n=10, figsize=15):\n",
        "    size = 8\n",
        "    scale = 1.0\n",
        "    figure = np.zeros((size * n, size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of chess configurations in the latent space\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = vae.decoder.predict(z_sample)\n",
        "            board = x_decoded[0][11].reshape(size, size)\n",
        "            figure[\n",
        "                i * size : (i + 1) * size,\n",
        "                j * size : (j + 1) * size,\n",
        "            ] = board\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = size // 2\n",
        "    end_range = n * size + start_range\n",
        "    pixel_range = np.arange(start_range, end_range, size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z_mean\")\n",
        "    plt.ylabel(\"z_log_var\")\n",
        "    plt.imshow(figure)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_latent_space(vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTd_PadymM7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning"
      ],
      "metadata": {
        "id": "9xSURRo-PDL5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3aTN_whzEeu"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_oyW6QfynBf"
      },
      "outputs": [],
      "source": [
        "mapper = {}\n",
        "mapper[\"p\"] = 0\n",
        "mapper[\"r\"] = 1\n",
        "mapper[\"n\"] = 2\n",
        "mapper[\"b\"] = 3\n",
        "mapper[\"q\"] = 4\n",
        "mapper[\"k\"] = 5\n",
        "mapper[\"P\"] = 0\n",
        "mapper[\"R\"] = 1\n",
        "mapper[\"N\"] = 2\n",
        "mapper[\"B\"] = 3\n",
        "mapper[\"Q\"] = 4\n",
        "mapper[\"K\"] = 5\n",
        "\n",
        "class Board(object):\n",
        "  def __init__(self, encoder, decoder, opposing_agent, FEN=None, capture_reward_factor=0.01):\n",
        "    \"\"\"\n",
        "    Chess Board Environment\n",
        "    Args:\n",
        "      FEN: str\n",
        "      Starting FEN notation, if None then start in the default chess position\n",
        "      capture_reward_factor: float [0,inf]\n",
        "      reward for capturing a piece. Multiply material gain by this number. 0 for normal chess.\n",
        "    \"\"\"\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.FEN = FEN\n",
        "    self.opposing_agent = opposing_agent\n",
        "    self.capture_reward_factor = capture_reward_factor\n",
        "    self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "    self.goal = self.generate_goal()\n",
        "    self.layer_board = self.init_layer_board()\n",
        "\n",
        "  def init_layer_board(self):\n",
        "    \"\"\"\n",
        "    Initalize the numerical representation of the environment\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    M = np.zeros(shape=(8, 8, 8))\n",
        "    for i in range(64):\n",
        "      row = i // 8\n",
        "      col = i % 8\n",
        "      piece = self.board.piece_at(i)\n",
        "      if piece == None:\n",
        "        continue\n",
        "      elif piece.symbol().isupper():\n",
        "        sign = 1\n",
        "      else:\n",
        "        sign = -1\n",
        "      layer = mapper[piece.symbol()]\n",
        "      M[layer, row, col] = sign\n",
        "      M[6, :, :] = 1 / self.board.fullmove_number\n",
        "    if self.board.turn:\n",
        "      M[6, 0, :] = 1\n",
        "    else:\n",
        "      M[6, 0, :] = -1\n",
        "    M[7, :, :] = 1\n",
        "    return np.concatenate((M, self.represent_goal()), axis=0)\n",
        "\n",
        "  def update_layer_board(self, move=None):\n",
        "      self._prev_layer_board = self.layer_board.copy()\n",
        "      self.init_layer_board()\n",
        "\n",
        "  def pop_layer_board(self):\n",
        "      self.layer_board = self._prev_layer_board.copy()\n",
        "      self._prev_layer_board = None\n",
        "\n",
        "  def generate_goal(self):\n",
        "    z_sample = np.random.normal(loc=0.0, scale=1.0, size=2)\n",
        "    return z_sample\n",
        "\n",
        "  def process_board(self):\n",
        "    \"\"\"\n",
        "    Process a board\n",
        "    \"\"\"\n",
        "    pieces = ['r', 'n', 'b', 'q', 'k', 'p'] + list(map(lambda x : x.upper(), ['r', 'n', 'b', 'q', 'k', 'p']))\n",
        "    list_arrays = [ np.eye(12)[:, i] for i in range(12) ]\n",
        "    encoding_dict = { k : v for k, v in zip(pieces, list_arrays) }\n",
        "    fen_list = self.board.fen().split()[0].split('/')\n",
        "    M = np.zeros((12, 8, 8))\n",
        "    for i, row in enumerate(fen_list) :\n",
        "      j = 0\n",
        "      for e in row :\n",
        "        if e.isdigit() :\n",
        "          j+= int(e)\n",
        "        else :\n",
        "          M[:, i, j] = encoding_dict[e]\n",
        "          j+=1\n",
        "    return  M\n",
        "\n",
        "  def step(self, action, test=True):\n",
        "    \"\"\"\n",
        "    Run a step\n",
        "    Args:\n",
        "      action: python chess move\n",
        "    Returns:\n",
        "      epsiode end: Boolean Whether the episode has ended\n",
        "      reward: float Difference in material value after the move\n",
        "    \"\"\"\n",
        "    # Play a move\n",
        "    self.board.push(action)\n",
        "    self.update_layer_board(action)\n",
        "    # Did the game end (checkmate or stalemate) or not yet\n",
        "    result = self.board.result()\n",
        "    if (result == '1-0') or (result == '0-1') or (result == '1/2-1/2'):\n",
        "      episode_end = True\n",
        "    else:\n",
        "      episode_end = False\n",
        "    # Compute the reward\n",
        "    processed_board = np.expand_dims(self.process_board(), 0) # .astype(\"float32\")\n",
        "    encoded_result = self.encoder.predict(processed_board)[0]\n",
        "    reward = -np.sqrt(np.sum((encoded_result-self.goal)**2))\n",
        "    return episode_end, reward\n",
        "\n",
        "  def get_random_action(self):\n",
        "    \"\"\"\n",
        "    Sample a random action\n",
        "    Returns: move A legal python chess move.\n",
        "    \"\"\"\n",
        "    legal_moves = [x for x in self.board.generate_legal_moves()]\n",
        "    legal_moves = np.random.choice(legal_moves)\n",
        "    return legal_moves\n",
        "\n",
        "  def project_legal_moves(self):\n",
        "    \"\"\"\n",
        "    Create a mask of legal actions\n",
        "    Returns: np.ndarray with shape (64,64)\n",
        "    \"\"\"\n",
        "    self.action_space = np.zeros(shape=(64, 64))\n",
        "    moves = [[x.from_square, x.to_square] for x in self.board.generate_legal_moves()]\n",
        "    for move in moves:\n",
        "      self.action_space[move[0], move[1]] = 1\n",
        "    return self.action_space\n",
        "\n",
        "  def get_material_value(self):\n",
        "    \"\"\"\n",
        "    Sums up the material balance using Reinfield values\n",
        "    Returns: The material balance on the board\n",
        "    \"\"\"\n",
        "    pawns = 1 * np.sum(self.layer_board[0, :, :])\n",
        "    rooks = 5 * np.sum(self.layer_board[1, :, :])\n",
        "    minor = 3 * np.sum(self.layer_board[2:4, :, :])\n",
        "    queen = 9 * np.sum(self.layer_board[4, :, :])\n",
        "    return pawns + rooks + minor + queen\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment\n",
        "    \"\"\"\n",
        "    self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "    self.layer_board = self.init_layer_board()\n",
        "\n",
        "  def sampled_board_threshold(self, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Sample a board using the decoder of the VAE\n",
        "    \"\"\"\n",
        "    x_decoded = (self.decoder.predict(np.expand_dims(self.goal, 0))[0] >= threshold).astype('int64')\n",
        "    board = [['.']*8 for i in range(8)]\n",
        "    pieces = ['r', 'n', 'b', 'q', 'k', 'p'] + list(map(lambda x : x.upper(), ['r', 'n', 'b', 'q', 'k', 'p']))\n",
        "    dict_pieces = {k:v for k, v in enumerate(pieces)}\n",
        "    for p in range(x_decoded.shape[0]):\n",
        "      for row in range(8):\n",
        "        for col in range(8):\n",
        "          if x_decoded[p, row, col] == 1:\n",
        "            board[row][col] = dict_pieces[p]\n",
        "    return board\n",
        "\n",
        "  def print_goal(self):\n",
        "    '''\n",
        "    Print the goal as a multi line string\n",
        "    '''\n",
        "    board = self.sampled_board_threshold()\n",
        "    for i in range(8):\n",
        "      biggus_stringus = ''\n",
        "      for j in range(8):\n",
        "        biggus_stringus += board[i][j] + '  '\n",
        "      print(biggus_stringus, end='\\n')\n",
        "\n",
        "  def represent_goal(self, threshold=0.5):\n",
        "    '''\n",
        "    Represent a latent goal as a (12, 8, 8) board\n",
        "    '''\n",
        "    return (self.decoder.predict(np.expand_dims(self.goal, 0))[0] >= threshold).astype('int64')\n",
        "\n",
        "  def set_goal(self):\n",
        "    '''\n",
        "    Set a goal for the enironment\n",
        "    '''\n",
        "    self.board = chess.Board(self.FEN) if self.FEN else chess.Board()\n",
        "    self.goal = self.generate_goal()\n",
        "    self.layer_board = self.init_layer_board()\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EGgNS1szHLZ"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIRMkwfQyost"
      },
      "outputs": [],
      "source": [
        "class GreedyAgent(object):\n",
        "\n",
        "    def __init__(self, color=-1):\n",
        "        self.color = color\n",
        "\n",
        "    def predict(self, layer_board, noise=True):\n",
        "        layer_board1 = layer_board[0, :, :, :]\n",
        "        pawns = 1 * np.sum(layer_board1[0, :, :])\n",
        "        rooks = 5 * np.sum(layer_board1[1, :, :])\n",
        "        minor = 3 * np.sum(layer_board1[2:4, :, :])\n",
        "        queen = 9 * np.sum(layer_board1[4, :, :])\n",
        "\n",
        "        maxscore = 40\n",
        "        material = pawns + rooks + minor + queen\n",
        "        board_value = self.color * material / maxscore\n",
        "        if noise:\n",
        "            added_noise = np.random.randn() / 1e3\n",
        "        return board_value + added_noise\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "\n",
        "    def __init__(self, lr=0.001, network='big'):\n",
        "        self.optimizer = Adam(learning_rate=lr)\n",
        "        self.model = Model()\n",
        "        self.proportional_error = False\n",
        "        if network == 'simple':\n",
        "            self.init_simple_network()\n",
        "        if network == 'big':\n",
        "            self.init_bignet()\n",
        "\n",
        "    def fix_model(self):\n",
        "        \"\"\"\n",
        "        The fixed model is the model used for bootstrapping\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "\n",
        "        self.fixed_model = clone_model(self.model)\n",
        "        self.fixed_model.compile(optimizer=self.optimizer, loss='mse', metrics=['mae'])\n",
        "        self.fixed_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def init_simple_network(self):\n",
        "\n",
        "        layer_state = Input(shape=(20, 8, 8), name='state')\n",
        "        conv1 = Conv2D(8, (3, 3), activation='sigmoid')(layer_state)\n",
        "        conv2 = Conv2D(6, (3, 3), activation='sigmoid')(conv1)\n",
        "        conv3 = Conv2D(4, (3, 3), activation='sigmoid')(conv2)\n",
        "        flat4 = Flatten()(conv3)\n",
        "        dense5 = Dense(24, activation='sigmoid')(flat4)\n",
        "        dense6 = Dense(8, activation='sigmoid')(dense5)\n",
        "        value_head = Dense(1)(dense6)\n",
        "\n",
        "        self.model = Model(inputs=layer_state,\n",
        "                           outputs=value_head)\n",
        "        self.model.compile(optimizer=self.optimizer,\n",
        "                           loss=mean_squared_error\n",
        "                           )\n",
        "\n",
        "    def init_bignet(self):\n",
        "        layer_state = Input(shape=(20, 8, 8), name='state')\n",
        "        conv_xs = Conv2D(4, (1, 1), activation='relu')(layer_state)\n",
        "        conv_s = Conv2D(8, (2, 2), strides=(1, 1), activation='relu')(layer_state)\n",
        "        conv_m = Conv2D(12, (3, 3), strides=(2, 2), activation='relu')(layer_state)\n",
        "        conv_l = Conv2D(16, (4, 4), strides=(2, 2), activation='relu')(layer_state)\n",
        "        conv_xl = Conv2D(20, (8, 8), activation='relu')(layer_state)\n",
        "        conv_rank = Conv2D(3, (1, 8), activation='relu')(layer_state)\n",
        "        conv_file = Conv2D(3, (8, 1), activation='relu')(layer_state)\n",
        "\n",
        "        f_xs = Flatten()(conv_xs)\n",
        "        f_s = Flatten()(conv_s)\n",
        "        f_m = Flatten()(conv_m)\n",
        "        f_l = Flatten()(conv_l)\n",
        "        f_xl = Flatten()(conv_xl)\n",
        "        f_r = Flatten()(conv_rank)\n",
        "        f_f = Flatten()(conv_file)\n",
        "\n",
        "        dense1 = Concatenate(name='dense_bass')([f_xs, f_s, f_m, f_l, f_xl, f_r, f_f])\n",
        "        dense2 = Dense(256, activation='sigmoid')(dense1)\n",
        "        dense3 = Dense(128, activation='sigmoid')(dense2)\n",
        "        dense4 = Dense(56, activation='sigmoid')(dense3)\n",
        "        dense5 = Dense(64, activation='sigmoid')(dense4)\n",
        "        dense6 = Dense(32, activation='sigmoid')(dense5)\n",
        "\n",
        "        value_head = Dense(1)(dense6)\n",
        "\n",
        "        self.model = Model(inputs=layer_state,\n",
        "                           outputs=value_head)\n",
        "        self.model.compile(optimizer=self.optimizer,\n",
        "                           loss=mean_squared_error\n",
        "                           )\n",
        "\n",
        "    def predict(self, board_layer):\n",
        "        return self.model.predict(board_layer)\n",
        "\n",
        "    def TD_update(self, states, rewards, sucstates, episode_active, gamma=0.9):\n",
        "        \"\"\"\n",
        "        Update the SARSA-network using samples from the minibatch\n",
        "        Args:\n",
        "            minibatch: list\n",
        "                The minibatch contains the states, moves, rewards and new states.\n",
        "        Returns:\n",
        "            td_errors: np.array\n",
        "                array of temporal difference errors\n",
        "        \"\"\"\n",
        "        suc_state_values = self.fixed_model.predict(sucstates)\n",
        "        V_target = np.array(rewards) + np.array(episode_active) * gamma * np.squeeze(suc_state_values)\n",
        "        # Perform a step of minibatch Gradient Descent.\n",
        "        self.model.fit(x=states, y=V_target, epochs=1, verbose=0)\n",
        "        V_state = self.model.predict(states)  # the expected future returns\n",
        "        td_errors = V_target - np.squeeze(V_state)\n",
        "\n",
        "        return td_errors\n",
        "\n",
        "    def MC_update(self, states, returns):\n",
        "        \"\"\"\n",
        "        Update network using a monte carlo playout\n",
        "        Args:\n",
        "            states: starting states\n",
        "            returns: discounted future rewards\n",
        "        Returns:\n",
        "            td_errors: np.array\n",
        "                array of temporal difference errors\n",
        "        \"\"\"\n",
        "        self.model.fit(x=states, y=returns, epochs=0, verbose=0)\n",
        "        V_state = np.squeeze(self.model.predict(states))\n",
        "        td_errors = returns - V_state\n",
        "\n",
        "        return td_errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pduagln90Krk"
      },
      "source": [
        "### Tree search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQDS4cS-z8CZ"
      },
      "outputs": [],
      "source": [
        "def softmax(x, temperature=1):\n",
        "    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))\n",
        "\n",
        "\n",
        "class Node(object):\n",
        "\n",
        "    def __init__(self, board=None, parent=None, gamma=0.9):\n",
        "        \"\"\"\n",
        "        Game Node for Monte Carlo Tree Search\n",
        "        Args:\n",
        "            board: the chess board\n",
        "            parent: the parent node\n",
        "            gamma: the discount factor\n",
        "        \"\"\"\n",
        "        self.children = {}  # Child nodes\n",
        "        self.board = board  # Chess board\n",
        "        self.parent = parent\n",
        "        self.values = []  # reward + Returns\n",
        "        self.gamma = gamma\n",
        "        self.starting_value = 0\n",
        "\n",
        "    def update_child(self, move, Returns):\n",
        "        \"\"\"\n",
        "        Update a child with a simulation result\n",
        "        Args:\n",
        "            move: The move that leads to the child\n",
        "            Returns: the reward of the move and subsequent returns\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        child = self.children[move]\n",
        "        child.values.append(Returns)\n",
        "\n",
        "    def update(self, Returns=None):\n",
        "        \"\"\"\n",
        "        Update a node with observed Returns\n",
        "        Args:\n",
        "            Returns: Future returns\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        if Returns:\n",
        "            self.values.append(Returns)\n",
        "\n",
        "    def select(self, color=1):\n",
        "        \"\"\"\n",
        "        Use Thompson sampling to select the best child node\n",
        "        Args:\n",
        "            color: Whether to select for white or black\n",
        "        Returns:\n",
        "            (node, move)\n",
        "            node: the selected node\n",
        "            move: the selected move\n",
        "        \"\"\"\n",
        "        assert color == 1 or color == -1, \"color has to be white (1) or black (-1)\"\n",
        "        if self.children:\n",
        "            max_sample = np.random.choice(color * np.array(self.values))\n",
        "            max_move = None\n",
        "            for move, child in self.children.items():\n",
        "                child_sample = np.random.choice(color * np.array(child.values))\n",
        "                if child_sample > max_sample:\n",
        "                    max_sample = child_sample\n",
        "                    max_move = move\n",
        "            if max_move:\n",
        "                return self.children[max_move], max_move\n",
        "            else:\n",
        "                return self, None\n",
        "        else:\n",
        "            return self, None\n",
        "\n",
        "    def simulate(self, model, env, depth=0, max_depth=2, random=False, temperature=1):\n",
        "        \"\"\"\n",
        "        Recursive Monte Carlo Playout\n",
        "        Args:\n",
        "            model: The model used for bootstrap estimation\n",
        "            env: the chess environment\n",
        "            depth: The recursion depth\n",
        "            max_depth: How deep to search\n",
        "            temperature: softmax temperature\n",
        "        Returns:\n",
        "            Playout result.\n",
        "        \"\"\"\n",
        "        board_in = env.board.fen()\n",
        "        if env.board.turn and random:\n",
        "            move = np.random.choice([x for x in env.board.generate_legal_moves()])\n",
        "        else:\n",
        "            successor_values = []\n",
        "            for move in env.board.generate_legal_moves():\n",
        "                episode_end, reward = env.step(move)\n",
        "                result = env.board.result()\n",
        "\n",
        "                if (result == \"1-0\" and env.board.turn) or (\n",
        "                        result == \"0-1\" and not env.board.turn):\n",
        "                    env.board.pop()\n",
        "                    env.init_layer_board()\n",
        "                    break\n",
        "                else:\n",
        "                    if env.board.turn:\n",
        "                        sucval = reward + self.gamma * np.squeeze(\n",
        "                            model.predict(np.expand_dims(env.layer_board, axis=0)))\n",
        "                    else:\n",
        "                        sucval = np.squeeze(env.opposing_agent.predict(np.expand_dims(env.layer_board, axis=0)))\n",
        "                    successor_values.append(sucval)\n",
        "                    env.board.pop()\n",
        "                    env.init_layer_board()\n",
        "\n",
        "            if not episode_end:\n",
        "                if env.board.turn:\n",
        "                    move_probas = softmax(np.array(successor_values), temperature=temperature)\n",
        "                    moves = [x for x in env.board.generate_legal_moves()]\n",
        "                else:\n",
        "                    move_probas = np.zeros(len(successor_values))\n",
        "                    move_probas[np.argmax(successor_values)] = 1\n",
        "                    moves = [x for x in env.board.generate_legal_moves()]\n",
        "                if len(moves) == 1:\n",
        "                    move = moves[0]\n",
        "                else:\n",
        "                    move = np.random.choice(moves, p=np.squeeze(move_probas))\n",
        "\n",
        "        episode_end, reward = env.step(move)\n",
        "\n",
        "        if episode_end:\n",
        "            Returns = reward\n",
        "        elif depth >= max_depth:  # Bootstrap the Monte Carlo Playout\n",
        "            Returns = reward + self.gamma * np.squeeze(model.predict(np.expand_dims(env.layer_board, axis=0)))\n",
        "        else:  # Recursively continue\n",
        "            Returns = reward + self.gamma * self.simulate(model, env, depth=depth + 1,temperature=temperature)\n",
        "\n",
        "        env.board.pop()\n",
        "        env.init_layer_board()\n",
        "\n",
        "        board_out = env.board.fen()\n",
        "        assert board_in == board_out\n",
        "\n",
        "        if depth == 0:\n",
        "            return Returns, move\n",
        "        else:\n",
        "            noise = np.random.randn() / 1e6\n",
        "            return Returns + noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Lop59y0ObS"
      },
      "source": [
        "### TD search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiUKAA4hzRhz"
      },
      "outputs": [],
      "source": [
        "class TD_search(object):\n",
        "\n",
        "    def __init__(self, env, agent, gamma=0.9, search_time=1, memsize=2000, batch_size=256, temperature=1):\n",
        "        \"\"\"\n",
        "        Chess algorithm that combines bootstrapped monte carlo tree search with Q Learning\n",
        "        Args:\n",
        "            env: RLC chess environment\n",
        "            agent: RLC chess agent\n",
        "            gamma: discount factor\n",
        "            search_time: maximum time spent doing tree search\n",
        "            memsize: Amount of training samples to keep in-memory\n",
        "            batch_size: Size of the training batches\n",
        "            temperature: softmax temperature for mcts\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.agent = agent\n",
        "        self.tree = Node(self.env)\n",
        "        self.gamma = gamma\n",
        "        self.memsize = memsize\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.reward_trace = []  # Keeps track of the rewards\n",
        "        self.piece_balance_trace = []  # Keep track of the material value on the board\n",
        "        self.ready = False  # Whether to start training\n",
        "        self.search_time = search_time\n",
        "        self.min_sim_count = 10\n",
        "\n",
        "        self.mem_state = np.zeros(shape=(1, 20, 8, 8))\n",
        "        self.mem_sucstate = np.zeros(shape=(1, 20, 8, 8))\n",
        "        self.mem_reward = np.zeros(shape=(1))\n",
        "        self.mem_error = np.zeros(shape=(1))\n",
        "        self.mem_episode_active = np.ones(shape=(1))\n",
        "\n",
        "    def learn(self, iters=40, c=5, timelimit_seconds=3600, maxiter=80):\n",
        "        \"\"\"\n",
        "        Start Reinforcement Learning Algorithm\n",
        "        Args:\n",
        "            iters: maximum amount of iterations to train\n",
        "            c: model update rate (once every C games)\n",
        "            timelimit_seconds: maximum training time\n",
        "            maxiter: Maximum duration of a game, in halfmoves\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        starttime = time.time()\n",
        "        for k in range(iters):\n",
        "            self.env.reset()\n",
        "            print(\"Goal:\")\n",
        "            self.env.print_goal()\n",
        "            if k % c == 0:\n",
        "                self.agent.fix_model()\n",
        "                print(\"iter\", k)\n",
        "            if k > c:\n",
        "                self.ready = True\n",
        "            self.play_game(k, maxiter=maxiter)\n",
        "            if starttime + timelimit_seconds < time.time():\n",
        "                break\n",
        "        return self.env.board\n",
        "\n",
        "    def play_game(self, k, maxiter=80):\n",
        "        \"\"\"\n",
        "        Play a chess game and learn from it\n",
        "        Args:\n",
        "            k: the play iteration number\n",
        "            maxiter: maximum duration of the game (halfmoves)\n",
        "        Returns:\n",
        "            board: Chess environment on terminal state\n",
        "        \"\"\"\n",
        "        episode_end = False\n",
        "        turncount = 0\n",
        "        tree = Node(self.env.board, gamma=self.gamma)  # Initialize the game tree\n",
        "\n",
        "        # Play a game of chess\n",
        "        while not episode_end:\n",
        "            state = np.expand_dims(self.env.layer_board.copy(), axis=0)\n",
        "            state_value = self.agent.predict(state)\n",
        "\n",
        "            # White's turn involves tree-search\n",
        "            if self.env.board.turn:\n",
        "\n",
        "                # Do a Monte Carlo Tree Search after game iteration k\n",
        "                start_mcts_after = -1\n",
        "                if k > start_mcts_after:\n",
        "                    tree = self.mcts(tree)\n",
        "                    # Step the best move\n",
        "                    max_move = None\n",
        "                    max_value = np.NINF\n",
        "                    for move, child in tree.children.items():\n",
        "                        sampled_value = np.mean(child.values)\n",
        "                        if sampled_value > max_value:\n",
        "                            max_value = sampled_value\n",
        "                            max_move = move\n",
        "                else:\n",
        "                    max_move = np.random.choice([move for move in self.env.board.generate_legal_moves()])\n",
        "                \n",
        "                print(\"White's turn:\")\n",
        "                print(self.env.board)\n",
        "\n",
        "            # Black's turn is myopic\n",
        "            else:\n",
        "                max_move = None\n",
        "                max_value = np.NINF\n",
        "                for move in self.env.board.generate_legal_moves():\n",
        "                    self.env.step(move)\n",
        "                    if self.env.board.result() == \"0-1\":\n",
        "                        max_move = move\n",
        "                        self.env.board.pop()\n",
        "                        self.env.init_layer_board()\n",
        "                        break\n",
        "                    successor_state_value_opponent = self.env.opposing_agent.predict(\n",
        "                        np.expand_dims(self.env.layer_board, axis=0))\n",
        "                    if successor_state_value_opponent > max_value:\n",
        "                        max_move = move\n",
        "                        max_value = successor_state_value_opponent\n",
        "\n",
        "                    self.env.board.pop()\n",
        "                    self.env.init_layer_board()\n",
        "\n",
        "                print(\"Black's turn:\")\n",
        "                print(self.env.board)\n",
        "\n",
        "            if not (self.env.board.turn and max_move not in tree.children.keys()) or not k > start_mcts_after:\n",
        "                tree.children[max_move] = Node(gamma=0.9, parent=tree)\n",
        "\n",
        "            episode_end, reward = self.env.step(max_move)\n",
        "            print('White Turn : ', self.env.board.turn)\n",
        "            print(f\"Reward at turn {turncount} : {reward}\")\n",
        "            print(f\"Maxmove {turncount} : {max_move}\")\n",
        "\n",
        "            tree = tree.children[max_move]\n",
        "            tree.parent = None\n",
        "            gc.collect()\n",
        "\n",
        "            sucstate = np.expand_dims(self.env.layer_board, axis=0)\n",
        "            new_state_value = self.agent.predict(sucstate)\n",
        "\n",
        "            error = reward + self.gamma * new_state_value - state_value\n",
        "            error = float(np.squeeze(error))\n",
        "\n",
        "            turncount += 1\n",
        "            if turncount > maxiter and not episode_end:\n",
        "                episode_end = True\n",
        "\n",
        "            episode_active = 0 if episode_end else 1\n",
        "\n",
        "            # construct training sample state, prediction, error\n",
        "            self.mem_state = np.append(self.mem_state, state, axis=0)\n",
        "            self.mem_reward = np.append(self.mem_reward, reward)\n",
        "            self.mem_sucstate = np.append(self.mem_sucstate, sucstate, axis=0)\n",
        "            self.mem_error = np.append(self.mem_error, error)\n",
        "            self.reward_trace = np.append(self.reward_trace, reward)\n",
        "            self.mem_episode_active = np.append(self.mem_episode_active, episode_active)\n",
        "\n",
        "            if self.mem_state.shape[0] > self.memsize:\n",
        "                self.mem_state = self.mem_state[1:]\n",
        "                self.mem_reward = self.mem_reward[1:]\n",
        "                self.mem_sucstate = self.mem_sucstate[1:]\n",
        "                self.mem_error = self.mem_error[1:]\n",
        "                self.mem_episode_active = self.mem_episode_active[1:]\n",
        "                gc.collect()\n",
        "\n",
        "            if turncount % 10 == 0:\n",
        "                self.update_agent()\n",
        "\n",
        "        piece_balance = self.env.get_material_value()\n",
        "        self.piece_balance_trace.append(piece_balance)\n",
        "        print(\"game ended with result\", reward, \"and material balance\", piece_balance, \"in\", turncount, \"halfmoves\")\n",
        "\n",
        "        return self.env.board\n",
        "\n",
        "    def update_agent(self):\n",
        "        \"\"\"\n",
        "        Update the Agent with TD learning\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if self.ready:\n",
        "            choice_indices, states, rewards, sucstates, episode_active = self.get_minibatch()\n",
        "            td_errors = self.agent.TD_update(states, rewards, sucstates, episode_active, gamma=self.gamma)\n",
        "            self.mem_error[choice_indices.tolist()] = td_errors\n",
        "\n",
        "    def get_minibatch(self, prioritized=True):\n",
        "        \"\"\"\n",
        "        Get a mini batch of experience\n",
        "        Args:\n",
        "            prioritized:\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        if prioritized:\n",
        "            sampling_priorities = np.abs(self.mem_error) + 1e-9\n",
        "        else:\n",
        "            sampling_priorities = np.ones(shape=self.mem_error.shape)\n",
        "        sampling_probs = sampling_priorities / np.sum(sampling_priorities)\n",
        "        sample_indices = [x for x in range(self.mem_state.shape[0])]\n",
        "        choice_indices = np.random.choice(sample_indices,\n",
        "                                          min(self.mem_state.shape[0],\n",
        "                                              self.batch_size),\n",
        "                                          p=np.squeeze(sampling_probs),\n",
        "                                          replace=False\n",
        "                                          )\n",
        "        states = self.mem_state[choice_indices]\n",
        "        rewards = self.mem_reward[choice_indices]\n",
        "        sucstates = self.mem_sucstate[choice_indices]\n",
        "        episode_active = self.mem_episode_active[choice_indices]\n",
        "\n",
        "        return choice_indices, states, rewards, sucstates, episode_active\n",
        "\n",
        "    def mcts(self, node):\n",
        "        \"\"\"\n",
        "        Run Monte Carlo Tree Search\n",
        "        Args:\n",
        "            node: A game state node object\n",
        "        Returns:\n",
        "            the node with playout sims\n",
        "        \"\"\"\n",
        "\n",
        "        starttime = time.time()\n",
        "        sim_count = 0\n",
        "        board_in = self.env.board.fen()\n",
        "\n",
        "        # First make a prediction for each child state\n",
        "        for move in self.env.board.generate_legal_moves():\n",
        "            if move not in node.children.keys():\n",
        "                node.children[move] = Node(self.env.board, parent=node)\n",
        "\n",
        "            episode_end, reward = self.env.step(move)\n",
        "\n",
        "            if episode_end:\n",
        "                successor_state_value = 0\n",
        "            else:\n",
        "                successor_state_value = np.squeeze(\n",
        "                    self.agent.model.predict(np.expand_dims(self.env.layer_board, axis=0))\n",
        "                )\n",
        "\n",
        "            child_value = reward + self.gamma * successor_state_value\n",
        "\n",
        "            node.update_child(move, child_value)\n",
        "            self.env.board.pop()\n",
        "            self.env.init_layer_board()\n",
        "        if not node.values:\n",
        "            node.values = [0]\n",
        "\n",
        "        while starttime + self.search_time > time.time() or sim_count < self.min_sim_count:\n",
        "            depth = 0\n",
        "            color = 1\n",
        "            node_rewards = []\n",
        "\n",
        "            # Select the best node from where to start MCTS\n",
        "            while node.children:\n",
        "                node, move = node.select(color=color)\n",
        "                if not move:\n",
        "                    # No move means that the node selects itself, not a child node.\n",
        "                    break\n",
        "                else:\n",
        "                    depth += 1\n",
        "                    color = color * -1  # switch color\n",
        "                    episode_end, reward = self.env.step(move)  # Update the environment to reflect the node\n",
        "                    node_rewards.append(reward)\n",
        "                    # Check best node is terminal\n",
        "\n",
        "                    if self.env.board.result() == \"1-0\" and depth == 1:  # -> Direct win for white, no need for mcts.\n",
        "                        self.env.board.pop()\n",
        "                        self.env.init_layer_board()\n",
        "                        node.update(1)\n",
        "                        node = node.parent\n",
        "                        return node\n",
        "                    elif episode_end:  # -> if the explored tree leads to a terminal state, simulate from root.\n",
        "                        while node.parent:\n",
        "                            self.env.board.pop()\n",
        "                            self.env.init_layer_board()\n",
        "                            node = node.parent\n",
        "                        break\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "            # Expand the game tree with a simulation\n",
        "            Returns, move = node.simulate(self.agent.fixed_model,\n",
        "                                          self.env,\n",
        "                                          temperature=self.temperature,\n",
        "                                          depth=0)\n",
        "            self.env.init_layer_board()\n",
        "\n",
        "            if move not in node.children.keys():\n",
        "                node.children[move] = Node(self.env.board, parent=node)\n",
        "\n",
        "            node.update_child(move, Returns)\n",
        "\n",
        "            # Return to root node and backpropagate Returns\n",
        "            while node.parent:\n",
        "                latest_reward = node_rewards.pop(-1)\n",
        "                Returns = latest_reward + self.gamma * Returns\n",
        "                node.update(Returns)\n",
        "                node = node.parent\n",
        "\n",
        "                self.env.board.pop()\n",
        "                self.env.init_layer_board()\n",
        "            sim_count += 1\n",
        "\n",
        "        board_out = self.env.board.fen()\n",
        "        assert board_in == board_out\n",
        "\n",
        "        return node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z3iZCUm_yze"
      },
      "source": [
        "# RIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MXjN1jC5Bvb"
      },
      "outputs": [],
      "source": [
        "class RIG_Algorithm(object):\n",
        "\n",
        "  def __init__(self, env, agent, TD_search, Node, vae, data, epochs_vae = 100, N_episodes = 100, N_steps = 100,  gamma=0.9, search_time=1, memsize=2000, batch_size=64):\n",
        "    \"\"\"\n",
        "    RIG algorithm\n",
        "    Args:\n",
        "    \"\"\"\n",
        "    self.env = env\n",
        "    self.agent = agent\n",
        "    self.TD_search = TD_search\n",
        "    self.Node = Node\n",
        "    self.vae = vae\n",
        "    self.data = data\n",
        "    self.epochs_vae = epochs_vae\n",
        "    self.N_episodes = N_episodes\n",
        "    self.N_steps = N_steps\n",
        "    self.gamma = gamma\n",
        "    self.search_time = search_time\n",
        "    self.memsize = memsize\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "    # 1 : collect D = {s(i)} using exploration policy : we have data \n",
        "    # 2 : Train VAE on D\n",
        "\n",
        "    self.vae.compile(optimizer=keras.optimizers.Adam())\n",
        "    self.vae.fit(self.data, epochs=self.epochs_vae, batch_size=self.batch_size) \n",
        "    # 3 : Fit prior p(z) to latent encodings : done in step 2\n",
        "    # 4 : \n",
        "\n",
        "    for n in range(self.N_episodes):\n",
        "      # Generate a new goal\n",
        "      #generated_goal = np.random.normal(loc=0.0, scale=1.0, size=2)\n",
        "      #self.TD_search.env.goal = generated_goal\n",
        "      #self.env.goal = generated_goal\n",
        "      self.TD_search.env.set_goal()\n",
        "\n",
        "      self.TD_search.learn(iters=self.N_steps, c=5, timelimit_seconds=180, maxiter=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution"
      ],
      "metadata": {
        "id": "C2DndVI1PfP9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ11PnvWTfkD"
      },
      "outputs": [],
      "source": [
        "vae = VAE(beta=1/10000)\n",
        "\n",
        "opponent = GreedyAgent()\n",
        "player = Agent(lr=0.01, network='simple')\n",
        "\n",
        "env = Board(vae.encoder, vae.decoder, opponent, FEN=None)\n",
        "\n",
        "learner = TD_search(env, player, gamma=0.99, search_time=2)\n",
        "node = Node(learner.env.board, gamma=learner.gamma)\n",
        "\n",
        "data = x_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obdLRV2wTn8Z"
      },
      "outputs": [],
      "source": [
        "rig = RIG_Algorithm(env, player, learner, node, vae, data, N_episodes = 10, N_steps = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFWBa2r7T3mP"
      },
      "outputs": [],
      "source": [
        "rig.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "34yzAtq5PkGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeR3S47Rai1V"
      },
      "outputs": [],
      "source": [
        "plt.plot(rig.TD_search.reward_trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98TLUpNQciy1"
      },
      "outputs": [],
      "source": [
        "reward_smooth = pd.DataFrame(rig.TD_search.reward_trace)\n",
        "reward_smooth.rolling(window=100, min_periods=0).mean().plot(figsize=(16, 9),\n",
        "                                                             title='average performance')\n",
        "plt.show()\n",
        "\n",
        "pgn = Game.from_board(learner.env.board)\n",
        "\n",
        "with open(\"rlc_pgn\", \"w\") as log:\n",
        "    log.write(str(pgn))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MiniRIG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}